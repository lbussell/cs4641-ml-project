<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <title>Predicting the Next Baseball Pitch</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="https://unpkg.com/sakura.css/css/sakura.css" type="text/css">
</head>

<body>
    <h1>Predicting the Next Pitch</h1>

    <strong>CS 4641 Spring 2021 Group 35 Project</strong>

    <p>Ali Ardestani, Logan Bussell, Jim James, Sreekar Madabushi, John Maginnes</p>

    <h4>Video Presentation</h4>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/3iw3mmttbJI" frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
        allowfullscreen></iframe>

    <h4><a href="https://github.com/lbussell/cs4641-ml-project">Project Source Code on GitHub</a></h4>

    <h2>Introduction</h2>
    <p>In baseball, there are a few distinct types of pitches that a pitcher can make, and it is tough to tell which one
        they will make at any given time. However, a pitcher isn’t a random number generator - they likely make certain
        pitches with higher probability than others, and change up their strategy depending on the state of the game.
        Knowing what pitch an opposing pitcher is going to make next can help to improve a batter’s accuracy.</p>
    <p>Hitting a baseball is very difficult. In fact, it is regularly considered the “hardest thing to do in sports.” A
        great batter can hit the ball only about 30% of the time. We would like to increase the chances of a Major
        League Baseball batter hitting the ball by predicting for them ahead of time what kind of pitch (fast ball,
        curve ball, etc) will be thrown next.</p>

    <h2>Problem Definition</h2>
    <p>The formal definition of our problem revolves heavily around the stats we believe to be useful in predicting the
        next pitch the pitcher intends to throw. We can summarize it as such: given the current pitch count (balls and
        strikes), previous pitch, current batter, amount of players on base, state of games (inning and strikes) and
        difference in score, our model will predict which pitch is thrown next for a given pitcher.</p>
    <p>In this particular problem definition, there are a few assumptions made. The first is that the distance between
        games is not a factor. The second is that the pitcher will have a similar pitching strategy throughout the
        entirety of the season.</p>

    <h2>Data Collection</h2>
    <p>Since 2015, all MLB stadiums have been equipped with a high-tech game tracking system known as Statcast. Statcast
        collects highly detailed data about player movements and athletic performance. For pitches, Statcast collects
        lots of information about ball release, extension, velocity, and spin rate. Using this information, Statcast
        uses its own neural network to classify the type of pitch that was just thrown. These labels are what we use to
        train our models with.</p>
    <p>Our team was able to make use of a library known as pybaseball (<a
            href="https://github.com/jldbc/pybaseball">GitHub</a>). This python package
        scrapes data Statcast data from several websites and provides it under one API, allowing developers to more
        easily access current and historical baseball data.</p>
    <p>After experimenting a bit with the package, we decided to focus on 3 pitchers for the purpose of our research,
        Gerrit Cole, Max Scherzer and Clayton Kershaw. We pulled all of their provided pitching data down into .csv
        files to be cleaned into what we felt was relevant.</p>

    <h2>Data Cleaning</h2>
    <p>The next step was to clean the data we retrieved from the package down to only statistics we saw as relevant for
        our model. The incoming data had 93 features, which we wanted to drastically simplify. The first thing we did
        was drop any rows that didn’t have a pitch label. These were typically games that were played in stadiums that
        did not utilize the PitchCast technology, and thus could be discarded. Next we discarded any data that we felt
        was totally irrelevant, such as the location of the game, or the names of the fielding players.</p>
    <p>After that, we consolidated the state of the game by keeping strictly numbers. For example, the dataset we were
        working with gave the names of every player on base, which we decided to drop, keeping only the number of
        runners on base. Similarly, we decided to drop the team scores and keep only a number representing how much the
        pitcher's team was winning or losing by.</p>
    <p>Next, we encode multiple different data points for ease of use. For the pitches, we put them in a binary format
        as either a fastball or not a fastball. This allowed us to remove more redundant data such as pitch speed. For
        the pitch count, we changed it from categorical data such as strike or ball to 1-hot encoding.</p>
    <p>Lastly, we created new columns to make it easier to predict the next pitch. In order to do this, we shifted
        relevant data from the previous pitch down to the next row. That way, our models would be able go one row at a
        time and know all the current game information as well as data about the previous pitch. In total, we ended with
        13 columns:</p>
    <ul>
        <li>Current # balls</li>
        <li>Current # strikes</li>
        <li>Current # outs</li>
        <li>Current Inning</li>
        <li>Number of batters on base</li>
        <li>Whether the previous pitch was a fastball or not</li>
        <li>Previous pitch horizontal and vertical position over the plate (x, z)</li>
        <li>Batter’s stance/handedness</li>
        <li>Whether the previous pitch was a ball/strike/hit (one-hot encoded, three columns)</li>
        <li>The score differential (pitching team runs minus batting team runs)</li>
    </ul>
    <p>In total, each pitcher had a slightly different amount of data depending on how many pitches total they threw.
    </p>
    <ul>
        <li>Gerrit Cole: 10,532 pitches</li>
        <li>Clayton Kershaw: 17,760 pitches</li>
        <li>Max Scherzer: 20,299 pitches</li>
    </ul>

    <h2>Methods and Results</h2>
    <p>We tried several different methods this time around, including an LSTM based neural network in PyTorch,
        RandomForest from SKlearn, and SVM from sklearn. For all of the models, we ran a grid search hyperparameter
        optimization, and then applied the model that performed the best on the validation set to the test set to get
        our accuracy metric.</p>
    <p>The data we used to train the methods was strictly from games before the data we used to test. This way, we
        simulated predicting pitches in the future.</p>
    <p>To put our results into perspective, we compared them against the naive guess of guessing fastball everytime. If
        done on our dataset, this would yield a 59.8% accuracy.</p>

    <h3>LSTM</h3>
    <h4>Method</h4>
    <p>For our LSTM, we tried a neural network with an LSTM layer, a few hidden layers with dropouts (to avoid
        overfitting) and various activations, and a final output layer with a log softmax. In an effort to achieve the
        best possible results, we tried numerous different LSTM layers, hidden layers, optimizers(SGD, Adam, AdamW,
        LBFGS), learning rates, and activation functions (ReLU, LeakyReLU, Tanh, SIgmoid).</p>
    <h4>Results</h4>
    <p>63.9% accuracy</p>
    <h4>Discussion</h4>
    <p>In our experimentation with different combinations of parameters, we did find that shallower networks tended to
        perform better. Those with fewer hidden layers and fewer LSTM layers tended to have better performance than
        those with more.</p>
    <p>That being said, the performance was still relatively poor compared to the naive guess. Our team speculates that
        there could be a few different reasons for this. While we did have a large amount of data, it may have still
        been insufficient for the quantity needed for an accurate neural net, leading to a slight underfit. In addition,
        The pitcher has a largely random component to their pitches, making the loss function very jagged. Small or no
        changes to the inputs can result in a different pitch. As a result, optimizers struggle to converge to a high
        performing local optimum.</p>

    <h3>Random Forest</h3>
    <h4>Method</h4>
    <p>Using the sklearn implementation of random forest, we were able to construct a random forest class that then
        enabled us to perform hyperparameter optimization across a variety of different parameters, including
        n_estimators (the number of decision trees), max_depth (the maximum depth of the decision tree), max_features
        (the number of features to consider when looking for the best split), as well as whether or not bootstrapping is
        used. This enabled us to identify optimal hyperparameters for random forest, which then achieved about 70%
        accuracy.</p>
    <h4>Results</h4>
    <p>70.2% Accuracy</p>
    <figure>
        <img src="./assets/feature_importance.png" alt="Random Forest Feature Importance" />
        <figcaption>Random forest feature importance</figcaption>
    </figure>
    <h4>Discussion</h4>
    <p>Our implementation of random forest achieved 70.2% accuracy. This performance was the best of all the different
        methods we employed. We noticed that smaller random forests that employ smaller decision trees were more
        effective computationally while achieving similar levels of accuracy. We were also able to identify the most
        important features in predicting the next pitches using the feature importances for any individual decision
        tree. </p>

    <h3>SVM</h3>
    <h4>Method</h4>
    <p>SVM stands for support vector machine. We employed the sklearn implementation of SVM to try and analyze if that
        method would improve upon the accuracy our random forest implementation had. The studies from the beginning of
        the project had told us that SVM would be the optimal method for predicting future data. We examined a variety
        of different parameters including the different kernels, between polynomial, linear, and rbf, as well as the
        regularization parameter C, in order to better our SVM method.</p>
    <h4>Results</h4>
    <p>62.2% Accuracy</p>
    <h4>Discussion</h4>
    <p>Unfortunately, we did not have enough time to run a hyperparameter optimization of SVM. Pre-existing literature
        tells us that SVM should be the most effective method for an optimization method like this one, but
        surprisingly, it was one of the least effective methods that we employed. This could be because we didn’t run
        the hyperparameter optimization or because the data was not optimal for an SVM method. In the future, it would
        be effective to run hyperparameter optimization for SVM and see if we could get better results. </p>

    <h2>Discussion</h2>
    <p>In conclusion, we saw varying levels of success with the different methods. While none of them were necessarily
        as high as we would have liked, we did see relatively good results with the Random Forest. In addition, each of
        them were able to successfully beat out the naive guess.</p>
    <p>Future improvements to this project could certainly be made by taking into account more data and data points. For
        example, we could attempt to classify results outside of just fastball and non-fastball, or look more
        specifically at the statistics of the batter.</p>

    <h4><a href="https://github.com/lbussell/cs4641-ml-project">Project Source Code on GitHub</a></h4>

    <h2>References and Similar Studies</h2>
    <ul>
        <li><a href="https://medium.com/@matt42kirby/predicting-baseball-pitches-33f906cbbdee">Predicting pitches for a
                single pitcher at 15% better than naive guess</a></li>
        <li><a
                href="https://medium.com/@alexwbell/predicting-the-next-baseball-pitch-utilizing-machine-learning-to-gain-an-advantage-cef35bbd6e26">Predicting
                the next baseball pitch</a></li>
        <li><a href="https://cs230.stanford.edu/projects_spring_2018/reports/8290890.pdf">Predicting Next Baseball Pitch
                Type with RNN</a></li>
        <li><a href="https://dash.harvard.edu/bitstream/handle/1/37364634/PLUNKETT-SENIORTHESIS-2019.pdf?sequence=1">Pitch
                Type Prediction in Major League Baseball</a></li>
        <li><a
                href="https://www.researchgate.net/publication/326972628_Applying_machine_learning_techniques_to_baseball_pitch_prediction">Applying
                Machine Learning Techniques to Baseball Pitch Prediction</a></li>
        <li><a href="https://fisher.wharton.upenn.edu/wp-content/uploads/2020/09/Thesis_Andrew-Cui.pdf">Forecasting
                Outcomes of Major League Baseball Games Using Machine Learning</a></li>
    </ul>

</body>

</html>